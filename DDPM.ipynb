{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_optimizer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meoyoQHI7tAm"
   },
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def extract(v, t, shape):\n",
    "        out = torch.gather(v, index=t, dim=0).float().to(t.device)\n",
    "        out = out.view([t.size(0)] + [1] * (len(shape) - 1)) # (Batch, 1, 1, 1, ...)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadImages(batch_size, path, size):\n",
    "        images = ImageFolder(path, transform=transforms.Compose([\n",
    "            transforms.Resize(int(size)),\n",
    "            transforms.RandomCrop(size),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        return DataLoader(images, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=os.cpu_count(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Model (DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拡散過程\n",
    "$\\epsilon_t 〜 N(0, I)\\quad(I: 単位行列)$ とし、\n",
    "$$x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_tt\\quad(\\beta: ノイズの大きさのハイパーパラメータ)$$\n",
    "とするマルコフ過程を考えると、 \n",
    "$$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$$\n",
    "$$∴x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}\\epsilon\\quad(\\alpha_t = 1 - \\beta_t, \\bar{\\alpha} = \\prod_{t=1}(\\alpha_t))$$\n",
    "また、$q(x_t | x_0) = N(x_t; \\sqrt{\\bar{\\alpha}}x_0, \\sqrt{1 - \\bar{\\alpha}}I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件付き逆拡散過程\n",
    "ベイズの定理とマルコフ性より、\n",
    "$$q(x_{t-1} | x_t, x_0) = \\frac{q(x_t | x_{t-1})q(x_{t-1} | x_0)}{q(x_t | x_0)}$$\n",
    "それぞれの条件付き確率を拡散過程の正規分布より求めると、\n",
    "$$q(x_{t-1} | x_t, x_0) ∝ exp[-\\frac{1}{2}\\frac{(x_{t-1} - \\bar{\\mu}_t(x_t, x_0))^2}{\\bar{\\beta}_t}]$$\n",
    "$$∴q(x_t | x_t, x_0) = N(x_{t-1}; \\bar{\\mu}_t(x_t, x_0), \\bar{\\beta}_tI)$$\n",
    "また、$$\\bar{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon)　…①$$\n",
    "$$\\bar{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}\\beta_t$$\n",
    "ここで、$\\sigma_t^2 = \\bar{\\beta}_t$とし、\n",
    "$$x_{t-1} = \\bar{\\mu}_t + \\bar{\\beta}_tz_t\\quad(z_t 〜 N(0, I))$$\n",
    "$$= \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_t) + \\sigma_t^2z_t$$\n",
    "$$= \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - α_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_t) + \\bar{\\beta}z_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "ニューラルネットワークのパラメータをθとすると、\n",
    "$$x_{t-1} = \\mu_\\theta(x_t, t) + \\sigma_t^2z_t\\quad(z_t 〜 N(0, I))$$\n",
    "ここで、画像生成のため、平均の負の対数尤度を考えると、\\\n",
    "マルコフ性と正規分布のカルバックライブラーダイバージェンスより、\n",
    "$$L_t = E_q[\\frac{1}{2\\sigma_t^2} || \\bar{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t) ||^2]$$\n",
    "①より、$\\mu_\\theta = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta)$と定義し、係数を簡略化すると、\n",
    "$$L_t^{simple} = E_q[||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_t, t) ||^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: [拡散モデルの基礎と研究事例: Imagen](https://qiita.com/iitachi_tdse/items/6cdd706efd0005c4a14a) \\\n",
    "参考: [DenoisingDiffusionProbabilityModel-ddpm-](https://github.com/zoubohao/DenoisingDiffusionProbabilityModel-ddpm-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM - 拡散過程\n",
    "class GaussianDiffusionTrainer(nn.Module):\n",
    "    def __init__(self, model, beta_1, beta_t, num_times):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.num_times = num_times\n",
    "\n",
    "        betas = nn.Parameter(torch.linspace(beta_1, beta_t, num_times).double())\n",
    "        \n",
    "        alphas = 1. - betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.sqrt_alphas_bar = nn.Parameter(torch.sqrt(alphas_bar))\n",
    "        self.sqrt_one_minus_alphas_bar = nn.Parameter(torch.sqrt(1. - alphas_bar))\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        t = torch.randint(self.num_times, size=(x_0.size(0),), device=x_0.device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        \n",
    "        x_t = (Util.extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0\n",
    "               + Util.extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise)\n",
    "        \n",
    "        loss = F.mse_loss(self.model(x_t, t), noise, reduction='none')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM - 条件付き逆拡散過程\n",
    "class GaussianDiffusionSampler(nn.Module):\n",
    "    def __init__(self, model, beta_1, beta_t, num_times):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.num_times = num_times\n",
    "        \n",
    "        betas = torch.linspace(beta_1, beta_t, num_times).double()\n",
    "        \n",
    "        alphas = 1. - betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:num_times]\n",
    "        \n",
    "        self.coeff1 = nn.Parameter(torch.sqrt(1. / alphas))\n",
    "        self.coeff2 = nn.Parameter(self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))\n",
    "        \n",
    "        self.var = nn.Parameter((1. - alphas_bar_prev) / (1. - alphas_bar) * betas)\n",
    "\n",
    "    def p_mean_variance(self, x_t, t):\n",
    "        eps = self.model(x_t, t)\n",
    "        xt_prev_mean = (Util.extract(self.coeff1, t, x_t.shape) * x_t\n",
    "                        - Util.extract(self.coeff2, t, x_t.shape) * eps)\n",
    "        var = Util.extract(self.var, t, x_t.shape)\n",
    "        return xt_prev_mean, var\n",
    "\n",
    "    def forward(self, x):\n",
    "        xt_shape = x.shape\n",
    "        for times in reversed(range(self.num_times)):\n",
    "            t = torch.ones([xt_shape[0]], dtype=torch.long, device=x.device) * times\n",
    "            x = x.detach()\n",
    "            t = t.detach()\n",
    "            mean, var= self.p_mean_variance(x, t)\n",
    "            if times > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = 0\n",
    "            x = mean + torch.sqrt(var) * noise\n",
    "            assert torch.isnan(x).int().sum() == 0, \"NaN in Tensor.\"\n",
    "        return torch.clip(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, num_times, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        assert dim_in % 2 == 0\n",
    "        \n",
    "        embedding = torch.arange(0, dim_in, step=2) / dim_in * math.log(10000)\n",
    "        embedding = torch.exp(-embedding)\n",
    "        position = torch.arange(num_times).float()\n",
    "        embedding = position.unsqueeze(0).T * embedding\n",
    "        embedding = torch.stack([torch.sin(embedding), torch.cos(embedding)], dim=-1)\n",
    "        embedding = embedding.view(num_times, dim_in)\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(embedding),\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Linear(dim_out, dim_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.embedding(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pointwise Convolution\n",
    "        self.query_conv = nn.Conv2d(dim_in, dim_in // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(dim_in, dim_in // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(dim_in, dim_in, kernel_size=1)\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x, return_map=False):\n",
    "        proj_query = self.query_conv(x).view(x.size(0), -1, x.size(2) * x.size(3)).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(x.size(0), -1, x.size(2) * x.size(3))\n",
    "        s = torch.bmm(proj_query, proj_key)\n",
    "        attention_map_T = F.softmax(s, dim=-2)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(x.size(0), -1, x.size(2) * x.size(3))\n",
    "        o = torch.bmm(proj_value, attention_map_T)\n",
    "        \n",
    "        o = o.view(x.size(0), x.size(1), x.size(2), x.size(3))\n",
    "        out = x + self.gamma * o\n",
    "        \n",
    "        if return_map:\n",
    "            return out, attention_map_T.permute(0, 2, 1)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_t, dropout=0., attention=False, num_groups=32):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim_in != dim_out:\n",
    "            self.shortcut = nn.Conv2d(dim_in, dim_out, kernel_size=1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.residual_1 = nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.time_projection = nn.Linear(dim_t, dim_out)\n",
    "        self.residual_2 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, dim_out),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(num_groups, dim_out)\n",
    "        )\n",
    "        if attention:\n",
    "            self.attention = SelfAttention(dim_out)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        residual_1 = self.residual_1(x)\n",
    "        time_projection = self.time_projection(t)[:, :, None, None]\n",
    "        \n",
    "        residual_2 = self.residual_2(residual_1 + time_projection)\n",
    "        \n",
    "        residual = F.mish(residual_2 + shortcut)\n",
    "        out = self.attention(residual)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_times, dim_hidden=128, mul_dim_list=[1, 2, 3, 4], attention_list=[2], num_resblocks=2, dropout=0., num_groups=32):\n",
    "        super().__init__()\n",
    "        assert all([i < len(mul_dim_list) for i in attention_list]), 'num_attention: index out of bounds.'\n",
    "        \n",
    "        dim_t = dim_hidden * 4\n",
    "        self.time_embedding = TimeEmbedding(num_times, dim_hidden, dim_t)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, dim_hidden, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(num_groups, dim_hidden),\n",
    "            nn.Mish(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.downblocks = nn.ModuleList()\n",
    "        downsample_list = []\n",
    "        dim = dim_hidden\n",
    "        for i, mul in enumerate(mul_dim_list):\n",
    "            dim_out = dim_hidden * mul\n",
    "            for _ in range(num_resblocks):\n",
    "                self.downblocks.append(ResidualBlock(dim, dim_out, dim_t, dropout=dropout, attention=(i in attention_list)))\n",
    "                dim = dim_out\n",
    "            self.downblocks.append(nn.Sequential(\n",
    "                nn.Conv2d(dim, dim, kernel_size=3, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, dim_out),\n",
    "                nn.Mish(inplace=True),\n",
    "            ))\n",
    "            downsample_list += [dim]\n",
    "\n",
    "        self.middleblocks = nn.Sequential(\n",
    "            ResidualBlock(dim, dim, dim_t, dropout=dropout, attention=True),\n",
    "            ResidualBlock(dim, dim, dim_t, dropout=dropout, attention=False),\n",
    "        )\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mul in reversed(list(enumerate(mul_dim_list))):\n",
    "            dim_out = dim_hidden * mul\n",
    "            dim_in = downsample_list.pop()\n",
    "            modules = nn.ModuleList()\n",
    "            modules.append(ResidualBlock(dim_in + dim, dim_out, dim_t, dropout=dropout, attention=(i in attention_list)))\n",
    "            for _ in range(num_resblocks):\n",
    "                modules.append(ResidualBlock(dim_out, dim_out, dim_t, dropout=dropout, attention=(i in attention_list)))\n",
    "            self.upblocks.append(modules)\n",
    "            self.upblocks.append(nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                nn.GroupNorm(num_groups, dim_out),\n",
    "                nn.Mish(inplace=True)\n",
    "            ))\n",
    "            dim = dim_out\n",
    "\n",
    "        self.tail = nn.Conv2d(dim_out, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = self.time_embedding(t)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        xs = []\n",
    "        for layer in self.downblocks:\n",
    "            if isinstance(layer, ResidualBlock):\n",
    "                x = layer(x, t)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                xs.append(x)\n",
    "        \n",
    "        for layer in self.middleblocks:\n",
    "            x = layer(x, t)\n",
    "\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, nn.ModuleList):\n",
    "                h = xs.pop()\n",
    "                x = torch.cat([x, h], dim=1)\n",
    "                for _layer in layer:\n",
    "                    x = _layer(x, t)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        has_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "        \n",
    "        self.args = args\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.model = UNet(self.args.num_times, dim_hidden=self.args.dim_hidden).to(self.device)\n",
    "        self.model.apply(self.weights_init)\n",
    "\n",
    "        self.diffusion_trainer = GaussianDiffusionTrainer(self.model, self.args.beta_1, self.args.beta_t, self.args.num_times).to(self.device)\n",
    "        self.diffusion_sampler = GaussianDiffusionSampler(self.model, self.args.beta_1, self.args.beta_t, self.args.num_times).to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=2 * self.args.lr)\n",
    "        self.optimizer = torch_optimizer.Lamb(self.model.parameters(), lr=2 * self.args.lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=4, eta_min=self.args.lr / 2)\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        self.dataloader = Util.loadImages(self.args.batch_size, self.args.image_dir, self.args.image_size)\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.model.cpu()\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.args.weight_dir, f'weights.{epoch}.pth'))\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weights.pth'):\n",
    "            self.model.load_state_dict(torch.load('weights.pth', map_location=self.device))\n",
    "            print('Loaded network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(args, resume=True):\n",
    "        if resume and os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                solver = load(f)\n",
    "                print('Loaded resume.')\n",
    "                return solver\n",
    "        else:\n",
    "            return Solver(args)\n",
    "    \n",
    "    def train(self, resume=True):\n",
    "        self.load_dataset()\n",
    "        \n",
    "        print(f'Use Device: {self.device}')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['Image Dir'] = self.args.image_dir\n",
    "        hyper_params['Result Dir'] = self.args.result_dir\n",
    "        hyper_params['Weight Dir'] = self.args.weight_dir\n",
    "        hyper_params['Image Size'] = self.args.image_size\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params[\"DDPM's beta_1\"] = self.args.beta_1\n",
    "        hyper_params[\"DDPM's beta_t\"] = self.args.beta_t\n",
    "        hyper_params[\"DDPM's Times\"] = self.args.num_times\n",
    "        hyper_params[\"UNet's dim_hidden\"] = self.args.dim_hidden\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['Num Train'] = self.args.num_train\n",
    "\n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for iters, (data, _) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                data = data.to(self.device, non_blocking=True)\n",
    "                \n",
    "                loss = self.diffusion_trainer(data).sum() / self.args.num_times\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Logging.\n",
    "                losses = {}\n",
    "                losses['loss'] = loss.item()\n",
    "                \n",
    "                epoch_loss += losses['loss']\n",
    "                #experiment.log_metrics(losses)\n",
    "\n",
    "            self.save_state(self.epoch)\n",
    "            print(f'Epoch[{self.epoch}]'\n",
    "                  + f' LR[{self.scheduler.get_last_lr()[0]:.5f}]'\n",
    "                  + f' Loss[{epoch_loss}]')\n",
    "\n",
    "            ## DEBUG\n",
    "            #self.generate()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if resume:\n",
    "                self.save_resume()\n",
    "    \n",
    "    def generate(self):\n",
    "        self.model.eval()\n",
    "        noise = torch.randn(size=[1, 3, self.args.image_size, self.args.image_size], device=self.device)\n",
    "        image = self.diffusion_sampler(noise)\n",
    "        save_image(image, os.path.join(self.args.result_dir, f'generated_{time.time()}.png'))\n",
    "        print('New picture was generated.')\n",
    "        self.model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver.load(args, resume=not args.noresume)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if args.generate:\n",
    "        solver.generate()\n",
    "        return\n",
    "    \n",
    "    solver.train(not args.noresume)\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_dir', type=str, default='')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--image_size', type=int, default=32)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--beta_1', type=float, default=0.0001)\n",
    "    parser.add_argument('--beta_t', type=float, default=0.02)\n",
    "    parser.add_argument('--num_times', type=int, default=1000)\n",
    "    parser.add_argument('--dim_hidden', type=int, default=128)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--num_train', type=int, default=100)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--generate', action='store_true')\n",
    "    #parser.add_argument('--noresume', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    args.noresume = True # Because, can not use for LAMB.\n",
    "\n",
    "    if len(unknown) != 0:\n",
    "        print(f'Unknown option: {unknown}')\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
